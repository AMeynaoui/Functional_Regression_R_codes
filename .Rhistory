predictions = predict(lasso.mod,newx=X,type="response") # calcule les prédictions données par le Lasso pour les valeurs de lambda de la grille (en colonnes) et tous les individus (en ligne)
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(ISLR)
data("Hitters")
head(Hitters)
Hitters <- na.omit(Hitters)
X = as.matrix(Hitters[,-c(14,15,19,20)])
ridge.mod = glmnet(X,Hitters$Salary, alpha=0)
lasso.mod = glmnet(X,Hitters$Salary, alpha=1)
ridge.mod$lambda
par(mfrow=c(1,2))
nbvar = ncol(X)
plot(ridge.mod,xvar='lambda',main="Ridge")
plot(lasso.mod,xvar='lambda',main='Lasso')
plot(lasso.mod$lambda,lasso.mod$df,type='s')
ridge.mod.cv = cv.glmnet(X,Hitters$Salary,alpha=0)
lasso.mod.cv = cv.glmnet(X,Hitters$Salary,alpha=1)
par(mfrow=c(1,2))
plot(ridge.mod.cv,main="Ridge") # trace l'erreur de validation croisée
plot(lasso.mod.cv,main="Lasso")
lambda.opt.ridge = ridge.mod.cv$lambda.min
coeffs.Ridge.cv = coef(ridge.mod.cv,s="lambda.min")
coeffs.Ridge.cv
lambda.opt.lasso = lasso.mod.cv$lambda.min
coeffs.lasso.cv = coef(lasso.mod.cv,s="lambda.min")
coeffs.lasso.cv
predictions = predict(lasso.mod,newx=X,type="response") # calcule les prédictions données par le Lasso pour les valeurs de lambda de la grille (en colonnes) et tous les individus (en ligne)
sigmahat2 = mean((predictions[,ncol(predictions)]-Hitters$Salary)^2)
BIC.crit = colSums((predictions-Hitters$Salary)^2)/sigmahat2+(log(nrow(Hitters))*lasso.mod$df
plot(BIC.crit,type='l')
predictions = predict(lasso.mod,newx=X,type="response") # calcule les prédictions données par le Lasso pour les valeurs de lambda de la grille (en colonnes) et tous les individus (en ligne)
sigmahat2 = mean((predictions[,ncol(predictions)]-Hitters$Salary)^2)
BIC.crit = colSums((predictions-Hitters$Salary)^2)/sigmahat2+log(nrow(Hitters))*lasso.mod$df
plot(BIC.crit,type='l')
lasso.mod$nulldev
lasso.mod$nulldev-deviance(lasso.mod)
colSums((predictions-Hitters$Salary)^2)/sigmahat2
phat = predict(lasso.mod,as.matrix(Xnewimp[,1:72]),type='response')
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(ISLR)
data("Hitters")
head(Hitters)
Hitters <- na.omit(Hitters)
X = as.matrix(Hitters[,-c(14,15,19,20)])
ridge.mod = glmnet(X,Hitters$Salary, alpha=0)
lasso.mod = glmnet(X,Hitters$Salary, alpha=1)
ridge.mod$lambda
par(mfrow=c(1,2))
nbvar = ncol(X)
plot(ridge.mod,xvar='lambda',main="Ridge")
plot(lasso.mod,xvar='lambda',main='Lasso')
plot(lasso.mod$lambda,lasso.mod$df,type='s')
ridge.mod.cv = cv.glmnet(X,Hitters$Salary,alpha=0)
lasso.mod.cv = cv.glmnet(X,Hitters$Salary,alpha=1)
par(mfrow=c(1,2))
plot(ridge.mod.cv,main="Ridge") # trace l'erreur de validation croisée
plot(lasso.mod.cv,main="Lasso")
lambda.opt.ridge = ridge.mod.cv$lambda.min
coeffs.Ridge.cv = coef(ridge.mod.cv,s="lambda.min")
coeffs.Ridge.cv
lambda.opt.lasso = lasso.mod.cv$lambda.min
coeffs.lasso.cv = coef(lasso.mod.cv,s="lambda.min")
coeffs.lasso.cv
predictions = predict(lasso.mod,newx=X,type="response") # calcule les prédictions données par le Lasso pour les valeurs de lambda de la grille (en colonnes) et tous les individus (en ligne)
sigmahat2 = mean((predictions[,ncol(predictions)]-Hitters$Salary)^2)
BIC.crit = colSums((predictions-Hitters$Salary)^2)/sigmahat2+log(nrow(Hitters))*lasso.mod$df
plot(BIC.crit,type='l')
lambda.BIC = lasso.mod$lambda[which.min(BIC.crit)]
lambda.BIC
coeffs.lasso.BIC = coef(lasso.mod,s=lambda.BIC)
coeffs.lasso.BIC
p = 0.3
n = nrow(Hitters)
Iapp = sample.int(n,round(n*(1-p))) # liste les indices des individus qui sont dans l'échantillon de test
Xtest = X[-Iapp,]
Xapp = X[Iapp,]
Ytest = Hitters$Salary[-Iapp]
Yapp = Hitters$Salary[Iapp]
ntest = length(Ytest)
napp = length(Yapp)
# Estimateur des moindres carrés
lm.mod.app = lm(Salary~.-NewLeague-League-Division,data=Hitters,subset=Iapp)
# Recalculer aussi, le Ridge et les estimateurs Lasso.
Ridge.mod.app = cv.glmnet(Xapp,Yapp,alpha = 0)
Lasso.mod.app = cv.glmnet(Xapp,Yapp,alpha = 1)
lasso.mod.tot = glmnet(Xapp,Yapp,alpha = 1)
predictions.app = predict(lasso.mod.tot,newx=Xapp,type="response")
sigmahat2.app = mean((predictions.app[,ncol(predictions.app)]-Yapp)^2)
BIC.crit.app = colSums((predictions.app-Yapp)^2)/sigmahat2.app+log(napp)*lasso.mod.tot$df
lambda.BIC.app = lasso.mod.tot$lambda[which.min(BIC.crit.app)]
MCO.pred = cbind(rep(1,ntest),Xtest) %*% lm.mod.app$coefficients
ylimites = range(c(MCO.pred,Ytest))
plot(Ytest,MCO.pred,pch=16,main="Estimateur des moindres carrés",xlab="Salaires réels des joueurs",ylab="Salaires prédits",xlim=ylimites,ylim=ylimites)
abline(c(0,1),col='red')
# Erreur moyenne de prediction
err.lm = mean((Ytest-MCO.pred)^2)
Ridge.pred = predict(Ridge.mod.app,newx=Xtest)
plot(Ytest,Ridge.pred,pch=16,main="Estimateur Ridge",xlab="Salaires réels des joueurs",ylab="Salaires prédits",xlim=ylimites,ylim=ylimites)
abline(c(0,1),col='red')
err.Ridge = mean((Ytest-Ridge.pred)^2)
Lasso.pred.cv = predict(Lasso.mod.app,newx=Xtest)
plot(Ytest,Lasso.pred.cv,pch=16,main="Estimateur Lasso (Validation croisée)",xlab="Salaires réels des joueurs",ylab="Salaires prédits",xlim=ylimites,ylim=ylimites)
abline(c(0,1),col='red')
err.Lasso.cv = mean((Ytest-Lasso.pred.cv)^2)
Lasso.pred.BIC = predict(Lasso.mod.app,s=lambda.BIC.app,newx=Xtest)
plot(Ytest,Lasso.pred.BIC,pch=16,main="Estimateur Lasso (BIC)",xlab="Salaires réels des joueurs",ylab="Salaires prédits",xlim=ylimites,ylim=ylimites)
abline(c(0,1),col='red')
err.Lasso.BIC = mean((Ytest-Lasso.pred.BIC)^2)
Erreurs = data.frame(err = c(abs(Ytest-MCO.pred),abs(Ytest-Ridge.pred),abs(Ytest-Lasso.pred.cv),abs(Ytest-Lasso.pred.BIC)),type=c(rep('MCO',ntest),rep('Ridge',ntest),rep('Lasso/CV',ntest),rep('Lasso/BIC',ntest)))
boxplot(err~type,data=Erreurs,main='Boxplot des erreurs de prédiction absolues')
abline(h=0,col='red')
library(readxl)
library(VIM)
library(glmnet)
Data_Cortex_Nuclear <- read_excel("Data_Cortex_Nuclear.xls")
X = matrix(NA,1080,77)
for (j in 2:78){
X[,(j-1)] = Data_Cortex_Nuclear[,j][[1]]
}
Y = Data_Cortex_Nuclear[,79][[1]]=='Control'
VIM.res = aggr(X,plot=F)
Xnew = X[,VIM.res$missings$Count<100]
Xnewimp = kNN(data.frame(Xnew))
fit_MLV = glm.fit(Xnewimp[,1:72],Y,family = binomial())
Ridge.mod = glmnet(as.matrix(Xnewimp[,1:72]),Y,family="binomial",alpha=0)
plot(Ridge.mod,xvar='lambda')
lasso.mod = glmnet(as.matrix(Xnewimp[,1:72]),Y,family="binomial",alpha=1)
plot(lasso.mod,xvar='lambda')
ridge.mod$lambda
plot(lasso.mod$lambda,lasso.mod$df,type='s')
ridge.mod.cv = cv.glmnet(as.matrix(Xnewimp[,1:72]),Y,alpha=0,family="binomial")
lasso.mod.cv = cv.glmnet(as.matrix(Xnewimp[,1:72]),Y,alpha=1,family="binomial")
par(mfrow=c(1,2))
plot(ridge.mod.cv,main="Ridge") # trace l'erreur de validation croisée
plot(lasso.mod.cv,main="Lasso")
lambda.opt.ridge = ridge.mod.cv$lambda.min
coeffs.Ridge.cv = coef(ridge.mod.cv,s="lambda.min")
coeffs.Ridge.cv
lambda.opt.lasso = lasso.mod.cv$lambda.min
coeffs.lasso.cv = coef(lasso.mod.cv,s="lambda.min")
coeffs.lasso.cv
phat = predict(lasso.mod,as.matrix(Xnewimp[,1:72]),type='response')
BIC.crit= colSums(Y*log(phat)+(1-Y)*log(1-phat))+log(length(Y))*lasso.mod$df
plot(lasso.mod$lambda,BIC.crit,type='b',main='Evolution du critere BIC')
phat = predict(lasso.mod,as.matrix(Xnewimp[,1:72]),type='response')
BIC.crit= colSums(Y*log(phat)+(1-Y)*log(1-phat))+log(length(Y))*lasso.mod$df
BIC.crit2 = -(lasso.mod$nulldev-deviance(lasso.mod))+log(length(Y))*lasso.mod$df
plot(lasso.mod$lambda,BIC.crit,type='b',main='Evolution du critere BIC')
phat = predict(lasso.mod,as.matrix(Xnewimp[,1:72]),type='response')
BIC.crit= colSums(Y*log(phat)+(1-Y)*log(1-phat))+log(length(Y))*lasso.mod$df
BIC.crit2 = -(lasso.mod$nulldev-deviance(lasso.mod))+log(length(Y))*lasso.mod$df
plot(lasso.mod$lambda,BIC.crit2,type='b',main='Evolution du critere BIC')
phat = predict(lasso.mod,as.matrix(Xnewimp[,1:72]),type='response')
BIC.crit= rowSums(Y*log(phat)+(1-Y)*log(1-phat))+log(length(Y))*lasso.mod$df
BIC.crit2 = -(lasso.mod$nulldev-deviance(lasso.mod))+log(length(Y))*lasso.mod$df
plot(lasso.mod$lambda,BIC.crit,type='b',main='Evolution du critere BIC')
phat = predict(lasso.mod,as.matrix(Xnewimp[,1:72]),type='response')
BIC.crit= colSums(Y*log(phat)+(1-Y)*log(1-phat))+log(length(Y))*lasso.mod$df
BIC.crit2 = -(lasso.mod$nulldev-deviance(lasso.mod))+log(length(Y))*lasso.mod$df
plot(lasso.mod$lambda,BIC.crit,type='b',main='Evolution du critere BIC')
phat = predict(lasso.mod,as.matrix(Xnewimp[,1:72]),type='response')
BIC.crit= colSums(Y*log(phat)+(1-Y)*log(1-phat))+log(length(Y))*lasso.mod$df
plot(lasso.mod$lambda,BIC.crit,type='b',main='Evolution du critere BIC')
phat = predict(lasso.mod,as.matrix(Xnewimp[,1:72]),type='response')
BIC.crit= colSums(Y*log(phat)+(1-Y)*log(1-phat))+log(length(Y))*lasso.mod$df/length(Y)
plot(lasso.mod$lambda,BIC.crit,type='b',main='Evolution du critere BIC')
phat = predict(lasso.mod,as.matrix(Xnewimp[,1:72]),type='response')
BIC.crit= -2*colSums(Y*log(phat)+(1-Y)*log(1-phat))+log(length(Y))*lasso.mod$df
plot(lasso.mod$lambda,BIC.crit,type='b',main='Evolution du critere BIC')
phat = predict(lasso.mod,as.matrix(Xnewimp[,1:72]),type='response')
BIC.crit= -2*colSums(Y*log(phat)+(1-Y)*log(1-phat))+log(length(Y))*lasso.mod$df
BIC.crit2 = -(lasso.mod$nulldev-deviance(lasso.mod))+log(length(Y))*lasso.mod$df
plot(lasso.mod$lambda,BIC.crit,type='b',main='Evolution du critere BIC')
phat = predict(lasso.mod,as.matrix(Xnewimp[,1:72]),type='response')
BIC.crit= -2*colSums(Y*log(phat)+(1-Y)*log(1-phat))+log(length(Y))*lasso.mod$df
BIC.crit2 = -(lasso.mod$nulldev-deviance(lasso.mod))+log(length(Y))*lasso.mod$df
plot(lasso.mod$lambda,BIC.crit2,type='b',main='Evolution du critere BIC')
phat = predict(lasso.mod,as.matrix(Xnewimp[,1:72]),type='response')
BIC.crit= -2*colSums(Y*log(phat)+(1-Y)*log(1-phat))+log(length(Y))*lasso.mod$df
plot(lasso.mod$lambda,BIC.crit,type='b',main='Evolution du critere BIC')
lambda.BIC = lasso.mod$lambda[which.min(BIC.crit)]
lambda.BIC
coeffs.lasso.BIC = coef(lasso.mod,s=lambda.BIC)
coeffs.lasso.BIC
p = 0.3
n = length(Y)
Iapp = sample.int(n,round(n*(1-p))) # liste les indices des individus qui sont dans l'échantillon de test
Xtest = as.matrix(Xnewimp[-Iapp,1:72])
Xapp = as.matrix(Xnewimp[Iapp,1:72])
Ytest = Y[-Iapp]
Yapp = Y[Iapp]
ntest = length(Ytest)
napp = length(Yapp)
# Recalculer aussi, le Ridge et les estimateurs Lasso.
Ridge.mod.app.prob = cv.glmnet(Xapp,Yapp,alpha = 0,family="binomial",type='response')
p = 0.3
n = length(Y)
Iapp = sample.int(n,round(n*(1-p))) # liste les indices des individus qui sont dans l'échantillon de test
Xtest = as.matrix(Xnewimp[-Iapp,1:72])
Xapp = as.matrix(Xnewimp[Iapp,1:72])
Ytest = Y[-Iapp]
Yapp = Y[Iapp]
ntest = length(Ytest)
napp = length(Yapp)
# Recalculer aussi, le Ridge et les estimateurs Lasso.
ridge.mod.cv.app = cv.glmnet(Xapp,Yapp,alpha = 0,family="binomial") # Ridge + CV
lasso.mod.cv.app = cv.glmnet(Xapp,Yapp,alpha = 1,family="binomial")  # Lasso + CV
lasso.mod.app = glmnet(Xapp,Yapp,alpha = 1,family="binomial")
phat.app = predict(lasso.mod.app,as.matrix(Xapp),type='response')
BIC.crit.app= -2*colSums(Yapp*log(phat.app)+(1-Yapp)*log(1-phat.app))+log(length(Yapp))*lasso.mod.app$df
ridge.mod.cv.app = cv.glmnet(Xapp,Yapp,alpha = 0,family="binomial") # Ridge + CV
lasso.mod.cv.app = cv.glmnet(Xapp,Yapp,alpha = 1,family="binomial")  # Lasso + CV
lasso.mod.app = glmnet(Xapp,Yapp,alpha = 1,family="binomial")
phat.app = predict(lasso.mod.app,as.matrix(Xapp),type='response')
BIC.crit.app= -2*colSums(Yapp*log(phat.app)+(1-Yapp)*log(1-phat.app))+log(length(Yapp))*lasso.mod.app$df
lambda.BIC.app = lasso.mod.app$lambda[which.min(BIC.crit.app)]
lasso.cv.proba = predict(lasso.mod.cv.app,newx=Xtest,s="lambda.min",type='response')
boxplot(lasso.cv.proba~Ytest)
lasso.cv.pred = lasso.cv.proba>=0.5
err.lasso.cv = mean((Ytest!=lasso.cv.pred))
FP.lasso.cv = mean(Ytest==0 & lasso.cv.pred==1)
FN.lasso.cv = mean(Ytest==1 & lasso.cv.pred==0)
# Lasso + CV
lasso.cv.app.proba = predict(lasso.mod.cv.app,newx=Xtest,s="lambda.min",type='response')
boxplot(lasso.cv.app.proba~Ytest)
lasso.cv.app.pred = lasso.cv.app.proba>=0.5
err.lasso.app.cv = mean((Ytest!=lasso.cv.app.pred))
FP.lasso.app.cv = mean(Ytest==0 & lasso.cv.app.pred==1)
FN.lasso.app.cv = mean(Ytest==1 & lasso.cv.pred==0)
# Lasso + BIC
lasso.BIC.app.proba = predict(lasso.mod.app,newx=Xtest,s=lambda.BIC.app,type='response')
boxplot(lasso.BIC.app.proba~Ytest)
lasso.BIC.app.pred = lasso.BIC.app.proba>=0.5
err.lasso.app.BIC = mean((Ytest!=lasso.BIC.app.pred))
FP.lasso.app.BIC = mean(Ytest==0 & lasso.BIC.app.pred==1)
FN.lasso.app.BIC = mean(Ytest==1 & lasso.BIC.app.pred==0)
# Lasso + CV
lasso.cv.app.proba = predict(lasso.mod.cv.app,newx=Xtest,s="lambda.min",type='response')
boxplot(lasso.cv.app.proba~Ytest,main="Estimation de p_i avec GLM Lasso + CV")
lasso.cv.app.pred = lasso.cv.app.proba>=0.5
err.lasso.app.cv = mean((Ytest!=lasso.cv.app.pred))
FP.lasso.app.cv = mean(Ytest==0 & lasso.cv.app.pred==1)
FN.lasso.app.cv = mean(Ytest==1 & lasso.cv.pred==0)
# Lasso + BIC
lasso.BIC.app.proba = predict(lasso.mod.app,newx=Xtest,s=lambda.BIC.app,type='response')
boxplot(lasso.BIC.app.proba~Ytest,main="Estimation de p_i avec GLM Lasso + BIC")
lasso.BIC.app.pred = lasso.BIC.app.proba>=0.5
err.lasso.app.BIC = mean((Ytest!=lasso.BIC.app.pred))
FP.lasso.app.BIC = mean(Ytest==0 & lasso.BIC.app.pred==1)
FN.lasso.app.BIC = mean(Ytest==1 & lasso.BIC.app.pred==0)
Ridge.pred.proba = predict(Ridge.mod.app,newx=Xtest,s="lambda.min",type='response')
?predict.glmnet
Ridge.pred.proba = predict(ridge.mod.cv.app,newx=Xtest,s="lambda.min",type='response')
boxplot(Ridge.pred.proba~Ytest,main="Estimation de p_i avec GLM RIdge")
# Lasso + CV
lasso.cv.app.proba = predict(lasso.mod.cv.app,newx=Xtest,s="lambda.min",type='response')
boxplot(lasso.cv.app.proba~Ytest,main="Estimation de p_i avec GLM Lasso + CV")
lasso.cv.app.pred = lasso.cv.app.proba>=0.5
err.lasso.app.cv = mean((Ytest!=lasso.cv.app.pred))
FP.lasso.app.cv = mean(Ytest==0 & lasso.cv.app.pred==1)
FN.lasso.app.cv = mean(Ytest==1 & lasso.cv.app.pred==0)
# Lasso + BIC
lasso.BIC.app.proba = predict(lasso.mod.app,newx=Xtest,s=lambda.BIC.app,type='response')
boxplot(lasso.BIC.app.proba~Ytest,main="Estimation de p_i avec GLM Lasso + BIC")
lasso.BIC.app.pred = lasso.BIC.app.proba>=0.5
err.lasso.app.BIC = mean((Ytest!=lasso.BIC.app.pred))
FP.lasso.app.BIC = mean(Ytest==0 & lasso.BIC.app.pred==1)
FN.lasso.app.BIC = mean(Ytest==1 & lasso.BIC.app.pred==0)
?plot.cv.glmnet
Ridge.pred = Ridge.pred.proba>=0.5
err.Ridge = mean((Ytest!=Ridge.pred))
FP.Ridge = mean(Ytest==0 & Ridge.pred==1)
FN.Ridge = mean(Ytest==1 & Ridge.pred==0)
print(paste("Erreur de classification du Ridge",err.Ridge))
Ridge.pred = Ridge.pred.proba>=0.5
err.Ridge = mean((Ytest!=Ridge.pred))
FP.Ridge = mean(Ytest==0 & Ridge.pred==1)
FN.Ridge = mean(Ytest==1 & Ridge.pred==0)
print(paste("Erreur de classification du Ridge",round(err.Ridge,2))
?round
Ridge.pred = Ridge.pred.proba>=0.5
err.Ridge = mean((Ytest!=Ridge.pred))
FP.Ridge = mean(Ytest==0 & Ridge.pred==1)
FN.Ridge = mean(Ytest==1 & Ridge.pred==0)
print(paste("Erreur de classification du Ridge",round(err.Ridge,2)))
Ridge.pred = Ridge.pred.proba>=0.5
err.Ridge = mean((Ytest!=Ridge.pred))
FP.Ridge = mean(Ytest==0 & Ridge.pred==1)
FN.Ridge = mean(Ytest==1 & Ridge.pred==0)
print(paste("Erreur de classification du Ridge",round(100*err.Ridge,2),"%"))
Ridge.pred = Ridge.pred.proba>=0.5
err.Ridge = mean((Ytest!=Ridge.pred))
FP.Ridge = mean(Ytest==0 & Ridge.pred==1)
FN.Ridge = mean(Ytest==1 & Ridge.pred==0)
print(paste("Erreur de classification du Ridge",round(100*err.Ridge,2),"%"))
print(paste("Taux de faux positifs",round(100*FP.Ridge,2),"%"))
print(paste("Taux de faux négatifs",round(100*FN.Ridge,2),"%"))
2+2*(4/5)
2+2*(5/4)
2.5*(10/12)
2.75*(5/4)
1+2,08+3,44
1+2.5*(10/12)+2.75*(5/4)
3+5.75*(10/12)+2.75*(5/4)
3.5+4.75*(10/12)+1*(5/4)
2+7*10/12
(2-4)/(3*4-2)
(2-6)/(3*6-2)
18*3
plot(sqrt)
x = seq(0.001,10,length=1000)
plot(x,sin(1/x),type='l')
plot(x,1-exp(-x),type='l')
abline(0,1,col='red')
x = seq(5,10,length=100)
plot(x,1-5/x,type='l')
x = seq(5,100,length=100)
plot(x,1-5/x,type='l')
p = seq(1/2,1/3,1/6)
p = c(1/2,1/3,1/6)
sum(p)
rmultinom(1,10,p)
rmultinom(1,100,p)
sin(0)
sin(pi/10)
sin(2*pi/10)
cos(0)
X
X = c(2.03, 2.11, 1.84, 1.89, 1.92, 2.12, 1.91, 1.99, 2.11, 2 )
mean(X)-sd(X)/sqrt(10)*1.96
mean(X)+sd(X)/sqrt(10)*1.96
sd(X)
qnorm(0.95)
qnorm(0.99)
qnorm(0.99)+1200/0.1
Delta<-qnorm(0.99)+1200/0.1
(-qnorm(0.99)+sqrt(Delta))/(2*sqrt(0.9/0.1))
(-qnorm(0.99)-sqrt(Delta))/(2*sqrt(0.9/0.1))
plot(qnorm)
(-qnorm(0.99)-sqrt(Delta))/(2*sqrt(0.9/0.1))->xp
xp^2
13/12000
1*10^(-4)
hatp = 13/12000
hatp-sqrt(1/(4*12000*0.05))
hatp+sqrt(1/(4*12000*0.05))
hatp-sqrt(hatp*(1-hatp)/n)*qnorm(0.975)
hatp+sqrt(hatp*(1-hatp)/n)*qnorm(0.975)
4.3-sqrt(6.76/100)*1.96
4.3+sqrt(6.76/100)*1.96
4.3-sqrt(6.76/100)*1.64
4.3+sqrt(6.76/100)*1.64
10.4+122+99+31.9
263.3+59.8
factorial(6)
720-36
684*6^(-2/3)/9
function(k){(1/k^2)*factorial(k)^(4/k-2)*(factorial(2*k)-factorial(k)^2)
}
function(k){(1/k^2)*factorial(k)^(4/k-2)*(factorial(2*k)-factorial(k)^2)}->coef
plot(1:10,coef(1:10))
function(k){(1/k^2)*factorial(k)^(4/k-2)*(factorial(2*k)-factorial(k)^2)}->CC
plot(1:10,CC(1:10))
CC<-function(k){(1/k^2)*factorial(k)^(4/k-2)*(factorial(2*k)-factorial(k)^2)}
plot(1:10,CC(1:10))
CC(1)
CC(2)
CC(10)
21.94+29.25+29.25+18+45
29.25+21.94+12+20.25+40.5+5+20
20.25+27+20
114+22.4
45+101.4
45+101.4+21.94+29.25+15.6+5.62+20
25.35/19.5
13.65/10.5
45+39
45+39+15.6/2
2-2*log(2)
log(2)
2**3
1.2*3.3+1
1.2*8+1
1.4+5+1.75
1.2*8.15+1
1.2*5+1
0.9+1.5+0.25
1.2*2.65+1
1.2*5.65+1
1.2*7.75+1
1.2*3.45+1
1.2*5.15+1
1.2*5.75+1
1.2*7.25+1
1.2*12.2+1
1.2*7+1
1.2*2.75+1
1.2*4.2+1
1.2*5.45+1
1.2*4.65+1
1.2*10.1+1
1.2*4.5+1
1.2*5.15+1
1.2*3.45+1
1.2*4.65+1
1.2*9.85+1
1.2*6.4+1
1.2*2.35+1
1.2*6.35+1
1.2*6.2+1
2.1+0.75+1.75+2.35
2.1+0.75+1.75
4.6*1.2+1
8*1.2+1
7.5*1.2+1
library("cluster", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
example(cluster)
example("kmeans")
plot(x,col=cl$cluster)
plot(x,col=cl$cluster,pch=16)
plot(x,col=cl$cluster,pch=16,cex=2)
plot(x,col=cl$cluster,pch=16,cex=1.2)
plot(x,pch=16)
plot(x,col=cl$cluster,pch=16,cex=1)
setwd("~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic")
source("PCA.R") # loading the functions for fPCA
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/main_simus.R')
install.packages("rstudioapi")
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/main_simus.R')
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/main_simus.R')
plot(simuX(1,100))
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/functions_simusXY.R')
plot(simuX(1,100))
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/functions_simusXY.R')
plot(simuX(1,100))
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/functions_simusXY.R')
plot(simuX(1,100))
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/functions_simusXY.R')
plot(simuX(1,100))
plot(as.vector(simuX(1,100)))
plot(as.vector(simuX(1,100)))
plot(as.vector(simuX(1,100)))
plot(as.vector(simuX(1,100)))
plot(as.vector(simuX(1,100,set='IK17')))
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/main_simus.R')
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/main_simus.R')
Xi = simuX(n,kreal=8,lambda = (pi*(0.5:(kreal-0.5)))^(-2))
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/main_simus.R')
View(simuX)
View(simuX)
View(simuX)
View(simuX)
View(simuX)
View(simuX)
Xi = simuX(n,kreal=8,lambda = (pi*(0.5:7.5))^(-2))
?Vectorize
Vectorize(cos)
?crossprod
?tcrossprod
Yi = simuY(X,operator,noise="Brownian",q=q,fac=20)
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/main_simus.R')
Yi = simuY(Xi,operator,noise="Brownian",q=q,fac=20)
mats = matrix(rep(seq(0,1,length.out=p),q),p,q)
p=2
q=3
mats = matrix(rep(seq(0,1,length.out=p),q),p,q)
mats
?matrix
matt = matrix(rep(t),p,p,q,byrow = T )
t = 1:q
matt = matrix(rep(t),p,p,q,byrow = T )
matt = matrix(rep(t,p),p,q,byrow = T )
matt
X %*% kernel(mats,matt)/p
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/main_simus.R')
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/main_simus.R')
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/Simus_Crambes&Mas2013.R')
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/main_simus.R')
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/main_simus.R')
plot(Y[1,],type='')
plot(Y[1,],type='l')
plot(Yi[1,],type='l')
plot(Yi[2,],type='l')
plot(Yi[3,],type='l')
Xi = simuX(n,lambda = (1:k1)^(-alpha/2),set="IK18",loi="unif")
alpha = 1.2
alpha = 1.2
k1 = 50
Xiii = simuX(n,lambda = (1:k1)^(-alpha/2),set="IK18",loi="unif")
plot(Xiii[i,],type='')
plot(Xiii[i,],type='l')
plot(Xiii[1,],type='l')
plot(Xiii[2,],type='l')
plot(Xiii[3,],type='l')
crossprod(1:3,seq(0,1,length=50))
tcrossprod(1:3,seq(0,1,length=50))
dim(tcrossprod(1:3,seq(0,1,length=50)))
Phip = sqrt(2)*cos(2*pi*tcrossprod(1:k1,gridX))
gridX = seq(0,1,length.out=p)
Phip = sqrt(2)*cos(2*pi*tcrossprod(1:k1,gridX))
beta = 3
gam = 2.5
B = 4*tcrossprod((-1)^(1:k1)*(1:k1)^gam,(-1)^(1:k1)*(1:k1)^beta)
B
B = 4*tcrossprod((-1)^(1:k1)*(1:k1)^(-gam),(-1)^(1:k1)*(1:k1)^(-beta))
head(B)
?crossprod
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/main_simus.R')
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/main_simus.R')
source('~/Dropbox/RegressionReponseFonctionnelle/Simus/SimusPublic/main_simus.R')
